{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e71527",
   "metadata": {},
   "source": [
    "# Mechanical Failure Risk Model (Baseline + Model Comparison)\n",
    "\n",
    "Goal: build an initial model that predicts the probability of **mechanical failure** after surgery, using:\n",
    "- **Pre-op patient measurements** (things we know before surgery)\n",
    "- **Plan variables** (things the optimizer can change: ALIF/TLIF/XLIF, implants, etc.)\n",
    "\n",
    "Outputs:\n",
    "- A baseline model (logistic regression) that returns a **risk probability**\n",
    "- A comparison against other models (RF, HistGB, XGBoost)\n",
    "- Saved artifacts so Vanja can plug the model into the optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9deff2",
   "metadata": {},
   "source": [
    "## 1) Load the dataset\n",
    "\n",
    "Load the Excel file into a dataframe and clean up column names.\n",
    "This avoids annoying issues later (extra spaces, line breaks inside column headers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "56cb56b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 93)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"../data/raw/MSDS_database_cleaned_deidentified_revised 1.xlsx\")\n",
    "df = pd.read_excel(DATA_PATH)\n",
    "\n",
    "df.columns = df.columns.astype(str).str.replace(\"\\n\",\" \").str.replace(r\"\\s+\",\" \", regex=True).str.strip()\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bcb581",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Check how many patients have mechanical failure vs no failure.\n",
    "Also check if there are any missing labels (we can’t train on those rows).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2bf790c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mech_fail_last\n",
       "0.0    90\n",
       "1.0    29\n",
       "NaN     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = \"mech_fail_last\"\n",
    "df[target].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7cac82",
   "metadata": {},
   "source": [
    "## 2) Define inputs (features) and output (target)\n",
    "\n",
    "We explicitly choose which columns the model is allowed to use:\n",
    "\n",
    "- **Predictors (pre-op):** patient demographics + pre-op alignment / bone quality proxies  \n",
    "- **Plan variables:** surgical “knobs” that the optimizer may change (ALIF/TLIF/XLIF, rods/screws, etc.)\n",
    "\n",
    "Then we build:\n",
    "- `X` = feature table (inputs)\n",
    "- `y` = target label (`mech_fail_last`)\n",
    "and drop any rows where `y` is missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "32571f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using n_features: 24\n",
      "Missing (not found in df): ['levels_fused']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((119, 24),\n",
       " mech_fail_last\n",
       " 0    90\n",
       " 1    29\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Feature list from data dictionary (pre-op predictors) ---\n",
    "PREDICTORS = [\n",
    "    \"age\", \"sex\",\n",
    "    \"PI_preop\", \"PT_preop\", \"LL_preop\", \"SS_preop\",\n",
    "    \"T4PA_preop\", \"L1PA_preop\", \"SVA_preop\",\n",
    "    \"cobb_main_curve_preop\", \"FC_preop\", \"tscore_femneck_preop\",\n",
    "    \"HU_UIV_preop\", \"HU_UIVplus1_preop\", \"HU_UIVplus2_preop\",\n",
    "    \"num_levels\",\n",
    "]\n",
    "\n",
    "# --- Plan / recipe variables (what optimizer will mutate) ---\n",
    "PLAN_COLS = [\n",
    "    \"UIV_implant\", \"levels_fused\", \"num_fused_levels\",\n",
    "    \"ALIF\", \"XLIF\", \"TLIF\",\n",
    "    \"num_rods\", \"num_screws\", \"osteotomy\",\n",
    "]\n",
    "\n",
    "target = \"mech_fail_last\"\n",
    "\n",
    "# Keep only columns that actually exist in the dataframe\n",
    "features = [c for c in (PREDICTORS + PLAN_COLS) if c in df.columns]\n",
    "missing = [c for c in (PREDICTORS + PLAN_COLS) if c not in df.columns]\n",
    "\n",
    "print(\"Using n_features:\", len(features))\n",
    "print(\"Missing (not found in df):\", missing)\n",
    "\n",
    "# Build X and y\n",
    "X = df[features].copy()\n",
    "y = df[target].copy()\n",
    "\n",
    "# Drop rows with missing target\n",
    "mask = y.notna()\n",
    "X = X.loc[mask].copy()\n",
    "y = y.loc[mask].astype(int)\n",
    "\n",
    "X.shape, y.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2defa48",
   "metadata": {},
   "source": [
    "## 3) Baseline model: logistic regression (with cross-validation)\n",
    "\n",
    "Start with a simple baseline: logistic regression.\n",
    "We set up a pipeline that:\n",
    "- fills missing values\n",
    "- scales numeric columns (helps logreg)\n",
    "- one-hot encodes categorical columns\n",
    "\n",
    "Then we generate **cross-validated probabilities** (each patient is predicted by a model that didn’t train on that patient).\n",
    "This gives us a more honest view of model behavior than training + testing on the same data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f0c14ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78484407, 0.22007698, 0.22457858, 0.71394014, 0.34797104,\n",
       "       0.42914456, 0.54416782, 0.13033537, 0.52303551, 0.28779948])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), num_cols),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]), cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "pipe_logreg = Pipeline([(\"preprocess\", preprocess), (\"model\", model)])\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "probs_logreg = cross_val_predict(pipe, X, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
    "probs_logreg[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c288b8",
   "metadata": {},
   "source": [
    "## 4) Evaluate baseline performance\n",
    "\n",
    "Two views of “how good is the model?”:\n",
    "\n",
    "1) **Ranking metrics (no threshold needed)**\n",
    "- ROC-AUC: how well the model separates failures from non-failures overall\n",
    "- PR-AUC (Average Precision): focuses more on identifying failures (useful when failures are less common)\n",
    "\n",
    "2) **Decision metrics (requires a threshold)**\n",
    "We pick a cutoff (default 0.5 here) to convert probabilities into yes/no predictions and compute:\n",
    "- precision, recall, F1\n",
    "- confusion matrix (TP/FP/TN/FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4eb0d929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.5739463601532567\n",
      "Avg Precision (PR-AUC): 0.32398384205601666\n",
      "Precision: 0.2826086956521739\n",
      "Recall: 0.4482758620689655\n",
      "F1: 0.3466666666666667\n",
      "{'tn': np.int64(57), 'fp': np.int64(33), 'fn': np.int64(16), 'tp': np.int64(13)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "auc = roc_auc_score(y, probs_logreg)\n",
    "ap  = average_precision_score(y, probs_logreg)\n",
    "\n",
    "preds = (probs_logreg >= 0.5).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y, preds).ravel()\n",
    "\n",
    "print(\"ROC-AUC:\", auc)\n",
    "print(\"Avg Precision (PR-AUC):\", ap)\n",
    "print(\"Precision:\", precision_score(y, preds, zero_division=0))\n",
    "print(\"Recall:\", recall_score(y, preds, zero_division=0))\n",
    "print(\"F1:\", f1_score(y, preds, zero_division=0))\n",
    "print({\"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d51ab",
   "metadata": {},
   "source": [
    "## 5) Inspect the highest-risk predictions\n",
    "\n",
    "Create a results table with:\n",
    "- predicted failure probability (cross-validated)\n",
    "- true outcome\n",
    "\n",
    "Then sort by predicted risk and look at the top patients.\n",
    "This is an easy way to sanity-check whether high-risk predictions are enriched with actual failures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1623f864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_fail_prob_cv</th>\n",
       "      <th>mech_fail_last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.988648</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.961685</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.950776</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.946098</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.942694</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.927672</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.910086</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.888469</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.886764</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.875918</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.838736</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.832906</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.821019</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.808593</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.784844</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pred_fail_prob_cv  mech_fail_last\n",
       "79            0.988648             0.0\n",
       "52            0.961685             1.0\n",
       "22            0.950776             1.0\n",
       "28            0.946098             0.0\n",
       "21            0.942694             0.0\n",
       "47            0.927672             1.0\n",
       "83            0.910086             0.0\n",
       "29            0.888469             0.0\n",
       "101           0.886764             1.0\n",
       "18            0.875918             0.0\n",
       "48            0.838736             0.0\n",
       "87            0.832906             1.0\n",
       "16            0.821019             0.0\n",
       "37            0.808593             0.0\n",
       "0             0.784844             0.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = df.loc[mask, :].copy()\n",
    "results[\"pred_fail_prob_cv\"] = probs_logreg\n",
    "\n",
    "results[[\"pred_fail_prob_cv\", \"mech_fail_last\"]].sort_values(\"pred_fail_prob_cv\", ascending=False).head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96f91f9",
   "metadata": {},
   "source": [
    "## 6) Threshold tradeoffs \n",
    "\n",
    "The model outputs probabilities, but if we want a yes/no “flag”, we need a threshold.\n",
    "\n",
    "Lower threshold catches more failures but creates more false alarms.  \n",
    "Higher threshold means fewer false alarms but misses more failures.\n",
    "\n",
    "This table helps decide an operating point later (if we ever need a hard cutoff).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e332ec9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>24</td>\n",
       "      <td>63</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>18</td>\n",
       "      <td>51</td>\n",
       "      <td>39</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>15</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.346667</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>57</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>66</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.264151</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>73</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  precision    recall        f1  tp  fp  tn  fn\n",
       "0        0.2   0.275862  0.827586  0.413793  24  63  27   5\n",
       "1        0.3   0.260870  0.620690  0.367347  18  51  39  11\n",
       "2        0.4   0.272727  0.517241  0.357143  15  40  50  14\n",
       "3        0.5   0.282609  0.448276  0.346667  13  33  57  16\n",
       "4        0.6   0.314286  0.379310  0.343750  11  24  66  18\n",
       "5        0.7   0.291667  0.241379  0.264151   7  17  73  22"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "thresholds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "rows = []\n",
    "for t in thresholds:\n",
    "    preds_t = (probs_logreg >= t).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, preds_t).ravel()\n",
    "    rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"precision\": precision_score(y, preds_t, zero_division=0),\n",
    "        \"recall\": recall_score(y, preds_t, zero_division=0),\n",
    "        \"f1\": f1_score(y, preds_t, zero_division=0),\n",
    "        \"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn\n",
    "    })\n",
    "\n",
    "pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c42dc5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base risk: 0.11319486752749376\n",
      "Toggled ALIF risk: 0.11319486752749376\n",
      "Change: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Fit on all data for demonstration (not evaluation)\n",
    "pipe_logreg.fit(X, y)\n",
    "\n",
    "# pick one patient row\n",
    "i = X.index[0]\n",
    "x0 = X.loc[[i]].copy()\n",
    "\n",
    "p_base = pipe_logreg.predict_proba(x0)[:, 1][0]\n",
    "\n",
    "# toggle ALIF if it exists\n",
    "x1 = x0.copy()\n",
    "if \"ALIF\" in x1.columns:\n",
    "    x1[\"ALIF\"] = 1 - int(x1[\"ALIF\"].iloc[0])\n",
    "\n",
    "p_new = pipe_logreg.predict_proba(x1)[:, 1][0]\n",
    "\n",
    "print(\"Base risk:\", p_base)\n",
    "print(\"Toggled ALIF risk:\", p_new)\n",
    "print(\"Change:\", p_new - p_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16e305a",
   "metadata": {},
   "source": [
    "## 7) Plan sensitivity test \n",
    "\n",
    "For optimization, it’s not enough to have a good predictor.\n",
    "We also need the risk score to **change** when we change plan variables.\n",
    "\n",
    "Here we do a simple what-if test:\n",
    "- take the same patient row\n",
    "- flip one plan knob like TLIF/ALIF/XLIF\n",
    "- see whether predicted risk changes\n",
    "\n",
    "If the risk doesn’t move, the optimizer can’t use that knob to rank plans.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "297ccc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALIF nonzero diffs: 0 avg abs change: 0.0\n",
      "TLIF nonzero diffs: 30 avg abs change: 0.5097843004143398\n",
      "XLIF nonzero diffs: 30 avg abs change: 0.009682999727552152\n"
     ]
    }
   ],
   "source": [
    "# Fit on all data for demonstration\n",
    "pipe_logreg.fit(X, y)\n",
    "\n",
    "def toggle_and_diff(col, n=10):\n",
    "    diffs = []\n",
    "    idxs = list(X.index)[:n]\n",
    "    for i in idxs:\n",
    "        x0 = X.loc[[i]].copy()\n",
    "        if col not in x0.columns:\n",
    "            continue\n",
    "        if pd.isna(x0[col].iloc[0]):\n",
    "            continue\n",
    "        try:\n",
    "            base = pipe_logreg.predict_proba(x0)[:,1][0]\n",
    "            x1 = x0.copy()\n",
    "            x1[col] = 1 - int(x1[col].iloc[0])  # assumes 0/1\n",
    "            new = pipe_logreg.predict_proba(x1)[:,1][0]\n",
    "            diffs.append(new - base)\n",
    "        except:\n",
    "            pass\n",
    "    return diffs\n",
    "\n",
    "for c in [\"ALIF\", \"TLIF\", \"XLIF\"]:\n",
    "    if c in X.columns:\n",
    "        d = toggle_and_diff(c, n=30)\n",
    "        print(c, \"nonzero diffs:\", sum(abs(x) > 1e-6 for x in d), \"avg abs change:\", np.mean(np.abs(d)) if d else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df873f0d",
   "metadata": {},
   "source": [
    "## 8) Turn the model into a callable risk function\n",
    "\n",
    "This is the interface the optimizer will use:\n",
    "for each candidate plan, it passes in patient features and plan values and gets back a risk probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7ca65f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted risk: 0.11319486752749376\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Fit model on all data (for scoring / optimizer use)\n",
    "pipe_logreg.fit(X, y)\n",
    "\n",
    "FEATURES = list(X.columns)\n",
    "\n",
    "def score_mech_fail(preop_plan_dict: dict) -> float:\n",
    "    \"\"\"\n",
    "    Input: dict with patient preop + plan fields (keys should match FEATURES).\n",
    "    Missing keys are allowed (treated as NaN and imputed).\n",
    "    Output: probability of mechanical failure (0..1).\n",
    "    \"\"\"\n",
    "    row = {c: preop_plan_dict.get(c, np.nan) for c in FEATURES}\n",
    "    X_new = pd.DataFrame([row], columns=FEATURES)\n",
    "    return float(pipe_logreg.predict_proba(X_new)[:, 1][0])\n",
    "\n",
    "#(uses an existing patient row)\n",
    "demo_dict = X.iloc[0].to_dict()\n",
    "print(\"Predicted risk:\", score_mech_fail(demo_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ab04c",
   "metadata": {},
   "source": [
    "## 9) Compare models (baseline vs tree/boosting methods)\n",
    "\n",
    "benchmark multiple models on the same features:\n",
    "- logistic regression\n",
    "- random forest\n",
    "- HistGradientBoosting\n",
    "- XGBoost\n",
    "\n",
    "report:\n",
    "- ROC-AUC and PR-AUC (prediction quality)\n",
    "- plan sensitivity metrics (does risk move when ALIF/TLIF/XLIF is flipped?)\n",
    "\n",
    "The goal is to find:\n",
    "- a strong predictive baseline, and\n",
    "- a model that is usable inside the optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b7480647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>ALIF_avg_abs_change</th>\n",
       "      <th>TLIF_avg_abs_change</th>\n",
       "      <th>XLIF_avg_abs_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.645785</td>\n",
       "      <td>0.330923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>histgb</td>\n",
       "      <td>0.601724</td>\n",
       "      <td>0.346827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.595402</td>\n",
       "      <td>0.289710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logreg</td>\n",
       "      <td>0.573946</td>\n",
       "      <td>0.323984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509784</td>\n",
       "      <td>0.009683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model   roc_auc    pr_auc  ALIF_avg_abs_change  TLIF_avg_abs_change  \\\n",
       "3     xgb  0.645785  0.330923                  0.0             0.000000   \n",
       "2  histgb  0.601724  0.346827                  0.0             0.000000   \n",
       "1      rf  0.595402  0.289710                  0.0             0.000000   \n",
       "0  logreg  0.573946  0.323984                  0.0             0.509784   \n",
       "\n",
       "   XLIF_avg_abs_change  \n",
       "3             0.000000  \n",
       "2             0.000000  \n",
       "1             0.009448  \n",
       "0             0.009683  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# XGBoost (now installed)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# -----------------------------\n",
    "# Preprocessing\n",
    "# -----------------------------\n",
    "cat_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
    "num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "\n",
    "preprocess_lr = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "                      (\"sc\", StandardScaler())]), num_cols),\n",
    "    (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                      (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols),\n",
    "])\n",
    "\n",
    "preprocess_tree = ColumnTransformer([\n",
    "    (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "    (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                      (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols),\n",
    "])\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "PLAN_VARS = [c for c in [\"ALIF\", \"TLIF\", \"XLIF\"] if c in X.columns]\n",
    "\n",
    "def plan_sensitivity(pipe, X, plan_vars=PLAN_VARS, n=30):\n",
    "    out = {}\n",
    "    idxs = list(X.index)[:min(n, len(X))]\n",
    "    for col in plan_vars:\n",
    "        diffs = []\n",
    "        for i in idxs:\n",
    "            x0 = X.loc[[i]].copy()\n",
    "            v = x0[col].iloc[0]\n",
    "            if pd.isna(v):\n",
    "                continue\n",
    "            try:\n",
    "                vi = int(v)\n",
    "            except:\n",
    "                continue\n",
    "            if vi not in (0, 1):\n",
    "                continue\n",
    "            p0 = float(pipe.predict_proba(x0)[:, 1][0])\n",
    "            x1 = x0.copy()\n",
    "            x1[col] = 1 - vi\n",
    "            p1 = float(pipe.predict_proba(x1)[:, 1][0])\n",
    "            diffs.append(p1 - p0)\n",
    "        diffs = np.array(diffs) if len(diffs) else np.array([])\n",
    "        out[col] = float(np.mean(np.abs(diffs))) if len(diffs) else 0.0\n",
    "    return out\n",
    "\n",
    "def eval_model(name, preprocess, clf):\n",
    "    pipe = Pipeline([(\"prep\", preprocess), (\"clf\", clf)])\n",
    "\n",
    "    # Honest cross-validated probabilities\n",
    "    probs = cross_val_predict(pipe, X, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
    "    roc = roc_auc_score(y, probs)\n",
    "    pr  = average_precision_score(y, probs)\n",
    "\n",
    "    # Fit on all data for plan sensitivity check\n",
    "    pipe.fit(X, y)\n",
    "    sens = plan_sensitivity(pipe, X)\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"roc_auc\": roc,\n",
    "        \"pr_auc\": pr,\n",
    "        **{f\"{k}_avg_abs_change\": v for k, v in sens.items()}\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Models to compare\n",
    "# -----------------------------\n",
    "results = []\n",
    "\n",
    "results.append(eval_model(\n",
    "    \"logreg\",\n",
    "    preprocess_lr,\n",
    "    LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "))\n",
    "\n",
    "results.append(eval_model(\n",
    "    \"rf\",\n",
    "    preprocess_tree,\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=600,\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        min_samples_leaf=2,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "))\n",
    "\n",
    "results.append(eval_model(\n",
    "    \"histgb\",\n",
    "    preprocess_tree,\n",
    "    HistGradientBoostingClassifier(\n",
    "        random_state=42,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.05,\n",
    "        max_iter=400\n",
    "    )\n",
    "))\n",
    "\n",
    "# XGBoost (conservative config for small n)\n",
    "results.append(eval_model(\n",
    "    \"xgb\",\n",
    "    preprocess_tree,\n",
    "    XGBClassifier(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=2,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_lambda=1.0,\n",
    "        min_child_weight=5,\n",
    "        random_state=42,\n",
    "        eval_metric=\"logloss\"\n",
    "    )\n",
    "))\n",
    "\n",
    "pd.DataFrame(results).sort_values(by=[\"roc_auc\", \"pr_auc\"], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c100e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit final models on full dataset (for saving)\n",
    "pipe_logreg = Pipeline([\n",
    "    (\"prep\", preprocess_lr),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\"))\n",
    "]).fit(X, y)\n",
    "\n",
    "pipe_xgb = Pipeline([\n",
    "    (\"prep\", preprocess_tree),\n",
    "    (\"clf\", XGBClassifier(\n",
    "        n_estimators=400, learning_rate=0.05, max_depth=2,\n",
    "        subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
    "        min_child_weight=5, random_state=42, eval_metric=\"logloss\"\n",
    "    ))\n",
    "]).fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "012939f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../artifacts/mech_fail_logreg.joblib\n",
      "Saved: ../artifacts/mech_fail_xgb.joblib\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import joblib, sklearn\n",
    "\n",
    "ARTIFACT_DIR = Path(\"../artifacts\")\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FEATURES = list(X.columns)\n",
    "\n",
    "def save_model(pipe_obj, name):\n",
    "    bundle = {\n",
    "        \"pipe\": pipe_obj,\n",
    "        \"features\": FEATURES,\n",
    "        \"target\": target,\n",
    "        \"sklearn_version\": sklearn.__version__,\n",
    "        \"model_name\": name,\n",
    "    }\n",
    "    out_path = ARTIFACT_DIR / f\"{name}.joblib\"\n",
    "    joblib.dump(bundle, out_path)\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "save_model(pipe_logreg, \"mech_fail_logreg\")\n",
    "save_model(pipe_xgb, \"mech_fail_xgb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d933c2",
   "metadata": {},
   "source": [
    "# Composite score model (predict post-op composite score from pre-op + plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9674c9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    119.000000\n",
       "mean      17.235060\n",
       "std       13.647377\n",
       "min        0.594468\n",
       "25%        5.997171\n",
       "50%       15.045995\n",
       "75%       27.003663\n",
       "max       74.247041\n",
       "Name: composite_score, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Jen's composite score calc \n",
    "df[\"gap_category_postop\"] = pd.cut(df[\"gap_score_postop\"], bins=[-np.inf, 2, 6, 13], labels=[\"P\", \"MD\", \"SD\"])\n",
    "\n",
    "def composite_score_calc(df, w1=1, w2=1, w3=1, w4=1, w5=1, w6=1):\n",
    "    weights = [w1, w2, w3, w4, w5, w6]\n",
    "    rel_weights = [w / sum(weights) for w in weights]\n",
    "\n",
    "    gap_score_postop = np.array(df[\"gap_score_postop\"] * 100 / 13)\n",
    "\n",
    "    l1pa_pen = [0 if abs(i) <= 3 else (abs(i) - 3) ** 2 for i in df[\"L1PA_ideal_mismatch_postop\"]]\n",
    "    l1pa_pen = np.array(l1pa_pen) / max(l1pa_pen) * 100 if max(l1pa_pen) > 0 else np.zeros(len(df))\n",
    "\n",
    "    l4s1_pen = [0 if 35 <= i <= 45 else (35 - i) ** 2 if i < 35 else (i - 45) ** 2 for i in df[\"L4_S1_postop\"]]\n",
    "    l4s1_pen = np.array(l4s1_pen) / max(l4s1_pen) * 100 if max(l4s1_pen) > 0 else np.zeros(len(df))\n",
    "\n",
    "    t4l1pa_pen = [0 if abs(i) <= 3 else (abs(i) - 3) ** 2 for i in df[\"T4L1PA_ideal_mismatch_postop\"]]\n",
    "    t4l1pa_pen = np.array(t4l1pa_pen) / max(t4l1pa_pen) * 100 if max(t4l1pa_pen) > 0 else np.zeros(len(df))\n",
    "\n",
    "    ll_ideal = 0.54 * df[\"PI_preop\"] + 27.6\n",
    "    ll_mismatch = df[\"LL_postop\"] - ll_ideal\n",
    "    ll_pen = [0 if abs(i) <= 3 else (abs(i) - 3) ** 2 for i in ll_mismatch]\n",
    "    ll_pen = np.array(ll_pen) / max(ll_pen) * 100 if max(ll_pen) > 0 else np.zeros(len(df))\n",
    "\n",
    "    gap_category_improvement = []\n",
    "    for i in range(len(df)):\n",
    "        if df[\"gap_category_postop\"].iloc[i] == \"P\" and df[\"gap_category\"].iloc[i] in [\"SD\", \"MD\", \"P\"]:\n",
    "            gap_category_improvement.append(0)\n",
    "        elif df[\"gap_category_postop\"].iloc[i] == \"MD\" and df[\"gap_category\"].iloc[i] == \"SD\":\n",
    "            gap_category_improvement.append(30)\n",
    "        else:\n",
    "            gap_category_improvement.append(100)\n",
    "    gap_category_improvement = np.array(gap_category_improvement)\n",
    "\n",
    "    composite = (\n",
    "        rel_weights[0] * gap_score_postop +\n",
    "        rel_weights[1] * l1pa_pen +\n",
    "        rel_weights[2] * l4s1_pen +\n",
    "        rel_weights[3] * t4l1pa_pen +\n",
    "        rel_weights[4] * ll_pen +\n",
    "        rel_weights[5] * gap_category_improvement\n",
    "    )\n",
    "    return composite\n",
    "\n",
    "df[\"composite_score\"] = composite_score_calc(df)\n",
    "df[\"composite_score\"].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aab10be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((119, 24),\n",
       " count    119.000000\n",
       " mean      17.235060\n",
       " std       13.647377\n",
       " min        0.594468\n",
       " 25%        5.997171\n",
       " 50%       15.045995\n",
       " 75%       27.003663\n",
       " max       74.247041\n",
       " Name: composite_score, dtype: float64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_comp = df[\"composite_score\"].copy()\n",
    "\n",
    "mask_comp = y_comp.notna()\n",
    "X_comp = df.loc[mask_comp, features].copy()\n",
    "y_comp = y_comp.loc[mask_comp].astype(float)\n",
    "\n",
    "X_comp.shape, y_comp.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c221e872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite score CV MAE: 5.9681600715948395\n",
      "Composite score CV RMSE: 8.19808468309337\n",
      "Composite score CV R2: 0.6360921807127342\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "cv_reg = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "pipe_comp = Pipeline([\n",
    "    (\"prep\", preprocess_tree),  # reuse your tree preprocessing\n",
    "    (\"reg\", HistGradientBoostingRegressor(\n",
    "        random_state=42, max_depth=3, learning_rate=0.05, max_iter=400\n",
    "    ))\n",
    "])\n",
    "\n",
    "pred_comp = cross_val_predict(pipe_comp, X_comp, y_comp, cv=cv_reg)\n",
    "\n",
    "mae  = mean_absolute_error(y_comp, pred_comp)\n",
    "mse  = mean_squared_error(y_comp, pred_comp)\n",
    "rmse = np.sqrt(mse)\n",
    "r2   = r2_score(y_comp, pred_comp)\n",
    "\n",
    "print(\"Composite score CV MAE:\", mae)\n",
    "print(\"Composite score CV RMSE:\", rmse)\n",
    "print(\"Composite score CV R2:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c2b401ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted composite score: 7.00100067799565\n"
     ]
    }
   ],
   "source": [
    "pipe_comp.fit(X_comp, y_comp)\n",
    "FEATURES_COMP = list(X_comp.columns)\n",
    "\n",
    "def score_composite(preop_plan_dict: dict) -> float:\n",
    "    row = {c: preop_plan_dict.get(c, np.nan) for c in FEATURES_COMP}\n",
    "    X_new = pd.DataFrame([row], columns=FEATURES_COMP)\n",
    "    return float(pipe_comp.predict(X_new)[0])\n",
    "\n",
    "demo_dict = X_comp.iloc[0].to_dict()\n",
    "print(\"Predicted composite score:\", score_composite(demo_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "70e7f791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../artifacts/composite_score_model.joblib\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import joblib, sklearn\n",
    "\n",
    "ARTIFACT_DIR = Path(\"../artifacts\")\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "bundle_comp = {\n",
    "    \"pipe\": pipe_comp,\n",
    "    \"features\": FEATURES_COMP,\n",
    "    \"target\": \"composite_score\",\n",
    "    \"sklearn_version\": sklearn.__version__,\n",
    "    \"model_name\": \"composite_histgb_reg\",\n",
    "}\n",
    "\n",
    "out_path = ARTIFACT_DIR / \"composite_score_model.joblib\"\n",
    "joblib.dump(bundle_comp, out_path)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
