{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5401e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src import config\n",
    "import src.optimization_utils as ou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d520fe89",
   "metadata": {},
   "source": [
    "### 1. Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Load cleaned data from config path\n",
    "df = pd.read_csv(config.DATA_PROCESSED)\n",
    "df.columns = df.columns.astype(str).str.replace(\"\\n\",\" \").str.replace(r\"\\s+\",\" \", regex=True).str.strip()\n",
    "\n",
    "print(f\"Loaded {len(df)} patients, {df.shape[1]} columns\")\n",
    "print(f\"Data path: {config.DATA_PROCESSED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6760f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change data type to numeric for calculation\n",
    "df[[\"gap_score_preop\", \"gap_score_postop\"]]\\\n",
    "    = df[[\"gap_score_preop\", \"gap_score_postop\"]]\\\n",
    "        .apply(pd.to_numeric, errors=\"coerce\").astype(\"Int64\")\n",
    "df[[\"ODI_preop\", \"ODI_12mo\"]]\\\n",
    "    = df[[\"ODI_preop\", \"ODI_12mo\"]]\\\n",
    "        .apply(pd.to_numeric, errors=\"coerce\").astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ae5688",
   "metadata": {},
   "source": [
    "### 1.1 Define input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0195072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#patient preop fixed parameters\n",
    "PATIENT_FIXED_COLS = config.PATIENT_FIXED_COLS\n",
    "FEATURES = config.DELTA_MODEL_FEATURES.copy()\n",
    "\n",
    "print(\"== FEATURES ==\")\n",
    "for i in FEATURES:\n",
    "    print(i)\n",
    "\n",
    "FEATURES.remove(\"gap_score_preop\")\n",
    "FEATURES.remove(\"gap_category\")\n",
    "\n",
    "FEATURES_CAT = [c for c in FEATURES if df[c].dtype == \"object\"]\n",
    "print(\"\\n == Categorical Features ==\")\n",
    "for i in FEATURES_CAT:\n",
    "    print(i)\n",
    "\n",
    "FEATURES_NUM = [c for c in FEATURES if df[c].dtype != \"object\"]\n",
    "print(\"\\n == Numerical Features (includes binary) ==\")\n",
    "for i in FEATURES_NUM:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf9c8cc",
   "metadata": {},
   "source": [
    "### 2. Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dae22da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_predict, cross_val_score, KFold, LeaveOneOut\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, PredictionErrorDisplay, make_scorer\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b218082",
   "metadata": {},
   "source": [
    "### 2.1 Set up for model building and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bcb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    "    )\n",
    "\n",
    "ridge_numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "           (\"scaler\", StandardScaler())]\n",
    "    )\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  \n",
    "           (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")) \n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Transform heterogeneous data types\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, FEATURES_NUM),\n",
    "        (\"cat\", categorical_transformer, FEATURES_CAT),\n",
    "        ]\n",
    "    )\n",
    "ridge_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", ridge_numeric_transformer, FEATURES_NUM),\n",
    "        (\"cat\", categorical_transformer, FEATURES_CAT),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "#hyperparameter tuning parameters\n",
    "param_distributions = {\"regressor__n_estimators\": [200, 300, 400, 500, 600],\n",
    "               \"regressor__max_depth\": [ 8, 12, 15, 20, None],\n",
    "               \"regressor__min_samples_split\" : [2, 5, 10, 15, 20],\n",
    "               \"regressor__min_samples_leaf\": [2, 4, 6, 8, 10], \n",
    "               \"regressor__max_features\": [\"sqrt\", \"log2\"]}\n",
    "\n",
    "ridge_param_distributions = {\"regressor__alpha\": [0.01, 0.1, 1, 10, 100, 1000],\n",
    "                             \"regressor__solver\": [\"auto\", \"svd\", \"cholesky\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6bbf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf_model(X_train, X_test, y_train, y_test):\n",
    "    \"Builds and fits a Random Forest model using a pipeline\"\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocessor\", clone(preprocessor)), \n",
    "        (\"regressor\", RandomForestRegressor(n_estimators=300, \n",
    "                                            random_state=42,\n",
    "                                            max_depth=8,\n",
    "                                            min_samples_leaf=5,\n",
    "                                            oob_score=True))\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    oob = pipeline.named_steps[\"regressor\"].oob_score_\n",
    "    \n",
    "    print(\"-- Random Forest Model --\")\n",
    "\n",
    "    print(f\"R² score: {round(r2,3)}\")\n",
    "    print(f\"RMSE: {round(rmse,3)}\")\n",
    "    print(f\"MAE: {round(mae, 3)}\")\n",
    "    print(f\"OOB score: {round(oob,3)}\")\n",
    "\n",
    "    return pipeline, y_pred\n",
    "                   \n",
    "def randomized_search_cv(pipeline, param_distributions, X_train,y_train, X_test,y_test):\n",
    "    \"Performs randomized search cross-validation on a model\"\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_distributions,\n",
    "        cv=5,\n",
    "        scoring=\"r2\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    #fit the model\n",
    "    random_search.fit(X_train,y_train)\n",
    "\n",
    "    #best paramaters\n",
    "    best_parameters = random_search.best_params_\n",
    "    best_cv_R2 = random_search.best_score_\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    #prediction\n",
    "    y_pred = random_search.predict(X_test)\n",
    "\n",
    "    #metrics\n",
    "    test_R2 = r2_score(y_test, y_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred)) \n",
    "\n",
    "    print(\"--Randomized search results--\")\n",
    "    print(\"Best parameters:\")\n",
    "    for k,v in best_parameters.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(f\"Best CV R2: {round(best_cv_R2,3)}\")\n",
    "    print(f\"Test R2: {round(test_R2,3)}\")\n",
    "    print(f\"Test RMSE: {round(test_rmse,3)}\")\n",
    "\n",
    "    return best_model, y_pred\n",
    "\n",
    "def train_xgboost_model(X_train, X_test, y_train, y_test):\n",
    "    \"Builds and fits an XGBoost model using a pipeline\"\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocessor\", clone(preprocessor)),\n",
    "        (\"regressor\", XGBRegressor(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42\n",
    "        ))\n",
    "        ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    print(\"-- XGBOOST Model --\")\n",
    "\n",
    "    print(f\"R² score: {round(r2,3)}\")\n",
    "    print(f\"RMSE: {round(rmse,3)}\")\n",
    "    print(f\"MAE: {round(mae, 3)}\")\n",
    "  \n",
    "    return pipeline, y_pred\n",
    "\n",
    "def train_ridge_model(X_train, X_test, y_train, y_test, alpha=1):\n",
    "    \"Builds and fits a Ridge Regression model within a pipeline\"\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocessor\", clone(ridge_preprocessor)),\n",
    "        (\"regressor\", Ridge(alpha=alpha))\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    print(\"-- Ridge Regression Model --\")\n",
    "\n",
    "    print(f\"R² score: {round(r2,3)}\")\n",
    "    print(f\"RMSE: {round(rmse,3)}\")\n",
    "    print(f\"MAE: {round(mae, 3)}\")\n",
    "\n",
    "    return pipeline, y_pred     \n",
    "\n",
    "#new pipeline for model comparison\n",
    "def compare_models(X, y, cv=5):\n",
    "    models = {\n",
    "        \"RF\": Pipeline([(\"preprocessor\", clone(preprocessor)), \n",
    "                        (\"regressor\", RandomForestRegressor(n_estimators=300, \n",
    "                                                            random_state=42, \n",
    "                                                            max_depth=8, \n",
    "                                                            min_samples_leaf=5))]),\n",
    "        \"XGBoost\": Pipeline([(\"preprocessor\", clone(preprocessor)), \n",
    "                             (\"regressor\", XGBRegressor(n_estimators=300, \n",
    "                                                        learning_rate=0.05, \n",
    "                                                        subsample=0.8, \n",
    "                                                        colsample_bytree=0.8, \n",
    "                                                        random_state=42))]),\n",
    "        \"Ridge\": Pipeline([(\"preprocessor\", clone(ridge_preprocessor)), \n",
    "                           (\"regressor\", Ridge())])\n",
    "    }\n",
    "    \n",
    "    print(\"\\nCross validation results:\")\n",
    "\n",
    "    for name, pipeline in models.items():\n",
    "        y_pred = cross_val_predict(pipeline, X, y, cv=cv)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        print(f\"\\n-- {name} --\")\n",
    "        print(f\"R² score: {round(r2,3)}\")\n",
    "        print(f\"RMSE: {round(rmse,3)}\")\n",
    "        print(f\"MAE: {round(mae,3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb14c5",
   "metadata": {},
   "source": [
    "### 3. Model Evaluation \n",
    "#### 3.1 Model 1: L4S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d196a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"delta_L4S1\"] = df[\"L4_S1_postop\"] - df[\"L4S1_preop\"]\n",
    "\n",
    "#drop null values — filter locally, don't mutate df\n",
    "df_L4S1 = df.dropna(subset=[\"delta_L4S1\"])\n",
    "\n",
    "X = df_L4S1[FEATURES]\n",
    "y = df_L4S1[\"delta_L4S1\"]\n",
    "\n",
    "#split into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b38197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple model comparision\n",
    "L4S1_rf_pipeline, L4S1_ypred\\\n",
    "      = train_rf_model(X_train, X_test, y_train, y_test)\n",
    "print()\n",
    "L4S1_xgb_pipeline, L4S1_xgb_ypred\\\n",
    "      = train_xgboost_model(X_train, X_test,y_train,y_test)\n",
    "print()\n",
    "#3 ridge regression - might be good for overfitting and correlated features\n",
    "L4S1_ridge_pipeline,L4S1_ridge_ypred\\\n",
    "      = train_ridge_model(X_train, X_test,y_train,y_test)\n",
    "\n",
    "#compare using cross validation\n",
    "compare_models(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomized search cross-validation to finetune Ridge regression parameters\n",
    "\n",
    "finetuned_L4S1_model, finetuned_L4S1_ypred  = \\\n",
    "    randomized_search_cv(L4S1_ridge_pipeline,ridge_param_distributions,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ec7876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of residuals\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle(\"L4S1 Ridge Regression Model\", fontsize=16)\n",
    "\n",
    "# Plot 1: Residual vs. Predicted (original model)\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    y_true=y_test, y_pred=L4S1_ridge_ypred, kind=\"residual_vs_predicted\", \n",
    "    ax=ax[0])\n",
    "ax[0].set_title(\"Residual vs. Predicted\")\n",
    "\n",
    "# Plot 2:  Residual histogram (original model)\n",
    "L4S1_ridge_residuals = y_test - L4S1_ridge_ypred\n",
    "sns.histplot(x=L4S1_ridge_residuals, kde=True, ax=ax[1], color='steelblue')\n",
    "ax[1].set_title(\"Residual Distribution\")\n",
    "ax[1].set_xlabel(\"Residuals\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bdb3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    y_true=y_test, \n",
    "    y_pred=L4S1_ridge_ypred, \n",
    "    kind=\"actual_vs_predicted\", \n",
    "    scatter_kwargs={\"alpha\": 0.5},\n",
    "    ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6369d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    y_true=y_test, \n",
    "    y_pred=L4S1_ridge_ypred, \n",
    "    kind=\"residual_vs_predicted\", \n",
    "    scatter_kwargs={\"alpha\": 0.5},\n",
    "    ax=ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1631f2",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ebe0e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshap\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "explainer = shap.Explainer(L4S1_ridge_pipeline.named_steps[\"regressor\"],\n",
    "                           feature_names=L4S1_ridge_pipeline.named_steps[\"preprocessor\"].get_feature_names_out())\n",
    "shap_values = explainer(X_test)\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a03f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31611414",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ec3da6",
   "metadata": {},
   "source": [
    "#### 3.2 Model 2: LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d516bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"delta_LL\"] = df[\"LL_postop\"] - df[\"LL_preop\"]\n",
    "\n",
    "df_LL = df.dropna(subset=[\"delta_LL\"])\n",
    "    \n",
    "X = df_LL[FEATURES]\n",
    "y = df_LL[\"delta_LL\"]\n",
    "\n",
    "#split into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010df10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model comparison\n",
    "LL_rf_pipeline, LL_ypred\\\n",
    "      = train_rf_model(X_train, X_test,y_train,y_test)\n",
    "print()\n",
    "\n",
    "LL_xgb_pipeline, LL_xgb_ypred\\\n",
    "      = train_xgboost_model(X_train, X_test,y_train,y_test)\n",
    "\n",
    "print()\n",
    "\n",
    "#3 ridge regression - might be good for overfitting and correlated features\n",
    "LL_ridge_pipeline, LL_ridge_ypred\\\n",
    "      = train_ridge_model(X_train, X_test,y_train,y_test)\n",
    "\n",
    "#compare using cross validation\n",
    "compare_models(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95199b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomized search cross-validation to finetune Ridge regression parameters\n",
    "\n",
    "finetuned_ll_model, finetuned_ll_ypred = \\\n",
    "    randomized_search_cv(LL_rf_pipeline, param_distributions,X_train,y_train,X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea27f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_ll_model, finetuned_ll_ypred = \\\n",
    "    randomized_search_cv(LL_ridge_pipeline, ridge_param_distributions,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e726c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of residuals\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle(\"LL Ridge Regression Model\", fontsize=16)\n",
    "\n",
    "# Plot 1: Residual vs. Predicted (original model)\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    y_true=y_test, y_pred=LL_ridge_ypred, kind=\"residual_vs_predicted\", \n",
    "    ax=ax[0,0])\n",
    "ax[0,0].set_title(\"Original Model: Residual vs. Predicted\")\n",
    "\n",
    "# Plot 2:  Residual histogram (original model)\n",
    "ll_ridge_residuals = y_test - LL_ridge_ypred\n",
    "sns.histplot(x=ll_ridge_residuals, kde=True, ax=ax[0,1], color='steelblue')\n",
    "ax[0, 1].set_title(\"Original Model: Residual Distribution\")\n",
    "ax[0, 1].set_xlabel(\"Residuals\")\n",
    "\n",
    "# Plot 3: Residual vs. Predicted (finetuned model)\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    y_true=y_test, y_pred=finetuned_ll_ypred, kind=\"residual_vs_predicted\", \n",
    "    ax=ax[1, 0]\n",
    ")\n",
    "ax[1, 0].set_title(\"Finetuned Model: Residual vs. Predicted\")\n",
    "\n",
    "# Plot 4: Residual Histogram (finetuned model)\n",
    "LL_ridge_cv_residuals = y_test - finetuned_ll_ypred\n",
    "sns.histplot(x=LL_ridge_cv_residuals, kde=True, ax=ax[1, 1], color='seagreen')\n",
    "ax[1, 1].set_title(\"Finetuned Model: Residual Distribution\")\n",
    "ax[1, 1].set_xlabel(\"Residuals\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a9ee9b",
   "metadata": {},
   "source": [
    "### 3.3 Model 3:T4PA Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b8b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"delta_T4PA\"] = df[\"T4PA_postop\"] - df[\"T4PA_preop\"]\n",
    "\n",
    "df_T4PA = df.dropna(subset=[\"delta_T4PA\"])\n",
    "    \n",
    "X = df_T4PA[FEATURES]\n",
    "y = df_T4PA[\"delta_T4PA\"]\n",
    "\n",
    "#split into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b87e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model comparison\n",
    "T4PA_rf_pipeline, T4PA_ypred\\\n",
    "      = train_rf_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print()\n",
    "T4PA_xgb_pipeline, T4PA_xgb_ypred\\\n",
    "      = train_xgboost_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print()\n",
    "\n",
    "T4PA_ridge_pipeline, T4PA_ridge_ypred\\\n",
    "      = train_ridge_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "#compare using cross validation\n",
    "compare_models(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b481616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomized search cross-validation to finetune Ridge regression parameters\n",
    "\n",
    "finetuned_T4PA_ridge, finetuned_T4PA_ypred  = \\\n",
    "    randomized_search_cv(T4PA_ridge_pipeline,ridge_param_distributions, X_train,y_train,X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce372cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of residuals\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle(\"T4PA Ridge Regression Model\", fontsize=16)\n",
    "\n",
    "# Plot 1: Residual vs. Predicted (original model)\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    y_true=y_test, y_pred=T4PA_ridge_ypred, kind=\"residual_vs_predicted\", \n",
    "    ax=ax[0,0])\n",
    "ax[0,0].set_title(\"Original Model: Residual vs. Predicted\")\n",
    "\n",
    "# Plot 2:  Residual histogram (original model)\n",
    "T4PA_ridge_residuals = y_test - T4PA_ridge_ypred\n",
    "sns.histplot(x=T4PA_ridge_residuals, kde=True, ax=ax[0,1], color='steelblue')\n",
    "ax[0, 1].set_title(\"Original Model: Residual Distribution\")\n",
    "ax[0, 1].set_xlabel(\"Residuals\")\n",
    "\n",
    "# Plot 3: Residual vs. Predicted (finetuned model)\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    y_true=y_test, y_pred=finetuned_T4PA_ypred, kind=\"residual_vs_predicted\", \n",
    "    ax=ax[1, 0]\n",
    ")\n",
    "ax[1, 0].set_title(\"Finetuned Model: Residual vs. Predicted\")\n",
    "\n",
    "# Plot 4: Residual Histogram (finetuned model)\n",
    "T4PA_cv_residuals = y_test - finetuned_T4PA_ypred\n",
    "sns.histplot(x=T4PA_cv_residuals, kde=True, ax=ax[1, 1], color='seagreen')\n",
    "ax[1, 1].set_title(\"Finetuned Model: Residual Distribution\")\n",
    "ax[1, 1].set_xlabel(\"Residuals\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e660dce9",
   "metadata": {},
   "source": [
    "### 3.4 Model 4:L1PA Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909df7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"delta_L1PA\"] = df[\"L1PA_postop\"] - df[\"L1PA_preop\"]\n",
    "\n",
    "df_L1PA = df.dropna(subset=[\"delta_L1PA\"])\n",
    "    \n",
    "X = df_L1PA[FEATURES]\n",
    "y = df_L1PA[\"delta_L1PA\"]\n",
    "\n",
    "#split into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bdb0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model comparison\n",
    "\n",
    "L1PA_rf_pipeline, L1PA_ypred\\\n",
    "      = train_rf_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print()\n",
    "\n",
    "L1PA_xgb_pipeline, L1PA_xgb_ypred\\\n",
    "      = train_xgboost_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print()\n",
    "\n",
    "L1PA_ridge_pipeline, L1PA_ridge_ypred\\\n",
    "      = train_ridge_model(X_train, X_test,y_train, y_test)\n",
    "\n",
    "compare_models(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44ef223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finetune random forest model\n",
    "finetuned_L1PA_pipeline, finetuned_L1PA_ypred= \\\n",
    "    randomized_search_cv(L1PA_rf_pipeline, param_distributions,X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9384eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of residuals\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle(\"L1PA Model Comparison\", fontsize=16)\n",
    "\n",
    "# Plot 1: Residual vs. Predicted (original model)\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    y_true=y_test, y_pred=L1PA_ypred, kind=\"residual_vs_predicted\", \n",
    "    ax=ax[0, 0])\n",
    "ax[0, 0].set_title(\"Original Model: Residual vs. Predicted\")\n",
    "\n",
    "# Plot 2:  Residual histogram (original model)\n",
    "L1PA_residuals = y_test - L1PA_ypred\n",
    "sns.histplot(x=L1PA_residuals, kde=True, ax=ax[0, 1], color='steelblue')\n",
    "ax[0, 1].set_title(\"Original Model: Residual Distribution\")\n",
    "ax[0, 1].set_xlabel(\"Residuals\")\n",
    "\n",
    "# Plot 3: Residual vs. Predicted (finetuned model)\n",
    "PredictionErrorDisplay.from_predictions(\n",
    "    y_true=y_test, y_pred=finetuned_L1PA_ypred, kind=\"residual_vs_predicted\", \n",
    "    ax=ax[1, 0]\n",
    ")\n",
    "ax[1, 0].set_title(\"Finetuned Model: Residual vs. Predicted\")\n",
    "\n",
    "# Plot 4: Residual Histogram (finetuned model)\n",
    "L1PA_cv_residuals = y_test - finetuned_L1PA_ypred\n",
    "sns.histplot(x=L1PA_cv_residuals, kde=True, ax=ax[1, 1], color='seagreen')\n",
    "ax[1, 1].set_title(\"Finetuned Model: Residual Distribution\")\n",
    "ax[1, 1].set_xlabel(\"Residuals\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd96cb7",
   "metadata": {},
   "source": [
    "#### 3.5 MODEL 5: ODI After 12 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f48391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"delta_ODI\"] = df[\"ODI_12mo\"] - df[\"ODI_preop\"] \n",
    "\n",
    "df_ODI = df.dropna(subset=[\"delta_ODI\"]) #many null rows\n",
    "    \n",
    "X = df_ODI[FEATURES]\n",
    "y = df_ODI[\"delta_ODI\"]\n",
    "\n",
    "#split into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3191c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model comparison\n",
    "\n",
    "ODI_rf_pipeline, ODI_ypred\\\n",
    "      = train_rf_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print()\n",
    "\n",
    "ODI_xgb_pipeline, ODI_xgb_ypred\\\n",
    "      = train_xgboost_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print()\n",
    "\n",
    "ODI_ridge_pipeline, ODI_ridge_ypred\\\n",
    "      = train_ridge_model(X_train, X_test,y_train, y_test)\n",
    "\n",
    "compare_models(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14904cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finetune ridge regression model\n",
    "finetuned_ODI_pipeline, finetuned_ODI_ypred= \\\n",
    "    randomized_search_cv(ODI_ridge_pipeline, ridge_param_distributions,X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca53d5",
   "metadata": {},
   "source": [
    "### 4. Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build final model for deployment - L4S1 ridge regression \n",
    "X = df_L4S1[FEATURES]\n",
    "y = df_L4S1[\"delta_L4S1\"]\n",
    "\n",
    "L4S1_pipe = Pipeline([\n",
    "        (\"preprocessor\", clone(ridge_preprocessor)),\n",
    "        (\"regressor\", Ridge(alpha=1))\n",
    "    ])\n",
    "\n",
    "L4S1_pipe.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de730dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build final model for deployment - LL random forest\n",
    "X = df_LL[FEATURES]\n",
    "y = df_LL[\"delta_LL\"]\n",
    "\n",
    "#change to random forest model \n",
    "LL_pipe = Pipeline([\n",
    "    (\"preprocessor\", clone(preprocessor)), \n",
    "    (\"regressor\", RandomForestRegressor(n_estimators=300, \n",
    "                                        random_state=42,\n",
    "                                        max_depth=8,\n",
    "                                        min_samples_leaf=5,\n",
    "                                        oob_score=True))])\n",
    "LL_pipe.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ebe88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build final model for deployment - T4PA ridge regression\n",
    "X = df_T4PA[FEATURES]\n",
    "y = df_T4PA[\"delta_T4PA\"]\n",
    "\n",
    "T4PA_pipe = Pipeline([\n",
    "        (\"preprocessor\", clone(ridge_preprocessor)),\n",
    "        (\"regressor\", Ridge(alpha=1))\n",
    "    ])\n",
    "\n",
    "T4PA_pipe.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53735b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build final model for deployment - L1PA ridge regression\n",
    "X = df_L1PA[FEATURES]\n",
    "y = df_L1PA[\"delta_L1PA\"]\n",
    "\n",
    "L1PA_pipe = Pipeline([\n",
    "        (\"preprocessor\", clone(ridge_preprocessor)),\n",
    "        (\"regressor\", Ridge(alpha=1))\n",
    "    ])\n",
    "\n",
    "L1PA_pipe.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa55974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build final model for deployment - ODI ridge regression\n",
    "X = df_ODI[FEATURES]\n",
    "y = df_ODI[\"delta_ODI\"]\n",
    "\n",
    "ODI_pipe = Pipeline([\n",
    "        (\"preprocessor\", clone(ridge_preprocessor)),\n",
    "        (\"regressor\", Ridge(alpha=1))\n",
    "    ])\n",
    "\n",
    "ODI_pipe.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ce731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib, sklearn\n",
    "from src import config\n",
    "\n",
    "# L4S1 model\n",
    "L4S1_dir = config.ARTIFACTS_DIR / \"L4S1\"\n",
    "L4S1_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "bundle_L4S1 = {\n",
    "    \"pipe\": L4S1_pipe,\n",
    "    \"features\": FEATURES,\n",
    "    \"target\": \"delta_L4S1\",\n",
    "    \"sklearn_version\": sklearn.__version__,\n",
    "    \"model_name\": \"RidgeRegressor_delta_L4S1\"\n",
    "}\n",
    "out_path = L4S1_dir / \"delta_L4S1_model.joblib\"\n",
    "joblib.dump(bundle_L4S1, out_path)\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "# LL model\n",
    "LL_dir = config.ARTIFACTS_DIR / \"LL\"\n",
    "LL_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "bundle_LL = {\n",
    "    \"pipe\": LL_pipe,\n",
    "    \"features\": FEATURES,\n",
    "    \"target\": \"delta_LL\",\n",
    "    \"sklearn_version\": sklearn.__version__,\n",
    "    \"model_name\": \"RandomForest_delta_LL\",\n",
    "}\n",
    "\n",
    "out_path = LL_dir / \"delta_LL_model.joblib\"\n",
    "joblib.dump(bundle_LL, out_path)\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "# T4PA model\n",
    "T4PA_dir = config.ARTIFACTS_DIR / \"T4PA\"\n",
    "T4PA_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "bundle_T4PA = {\n",
    "    \"pipe\": T4PA_pipe,\n",
    "    \"features\": FEATURES,\n",
    "    \"target\": \"delta_T4PA\",\n",
    "    \"sklearn_version\": sklearn.__version__,\n",
    "    \"model_name\": \"RidgeRegressor_delta_T4PA\",\n",
    "}\n",
    "\n",
    "out_path = T4PA_dir / \"delta_T4PA_model.joblib\"\n",
    "joblib.dump(bundle_T4PA, out_path)\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "# L1PA model\n",
    "L1PA_dir = config.ARTIFACTS_DIR / \"L1PA\"\n",
    "L1PA_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "bundle_L1PA = {\n",
    "    \"pipe\": L1PA_pipe,\n",
    "    \"features\": FEATURES,\n",
    "    \"target\": \"delta_L1PA\",\n",
    "    \"sklearn_version\": sklearn.__version__,\n",
    "    \"model_name\": \"RidgeRegressor_delta_L1PA\",\n",
    "}\n",
    "\n",
    "out_path = L1PA_dir / \"delta_L1PA_model.joblib\"\n",
    "joblib.dump(bundle_L1PA, out_path)\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "# ODI model\n",
    "ODI_dir = config.ARTIFACTS_DIR / \"ODI\"\n",
    "ODI_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "bundle_ODI = {\n",
    "    \"pipe\": ODI_pipe,\n",
    "    \"features\": FEATURES,\n",
    "    \"target\": \"delta_ODI\",\n",
    "    \"sklearn_version\": sklearn.__version__,\n",
    "    \"model_name\": \"RidgeRegressor_delta_ODI\",\n",
    "}\n",
    "\n",
    "out_path = ODI_dir / \"delta_ODI_model.joblib\"\n",
    "joblib.dump(bundle_ODI, out_path)\n",
    "print(\"Saved:\", out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
